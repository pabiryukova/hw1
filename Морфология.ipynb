{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_z2UbpbbHoSp"
   },
   "source": [
    "# Домашнее задание 1.\n",
    "Тема: Морфология"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epaet-oZHvHn"
   },
   "source": [
    "## Задание\n",
    "Обработать текст книги с помощью морфологического анализатора, сделать поверхностный анализ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rRspZE7CHmLh"
   },
   "source": [
    "### 1. Выбрать и сохранить книгу в .txt. Проследите, что кодировка UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Книга, покорившая мир, эталон литературы, синоним успеха. Книга, сделавшая Дж.К. Роулинг самым читаемым писателем современности. Книга, ставшая культовой уже для нескольких поколений. «Гарри Поттер и философский камень» — история начинается. Для среднего школьного возраста.\n"
     ]
    }
   ],
   "source": [
    "with open('garri_potter.txt', 'r') as infile:\n",
    "    text = infile.read().replace('Annotation', '')\n",
    "\n",
    "annotation = text[:280].strip()\n",
    "print(annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sir1m2MYH_HD"
   },
   "source": [
    "### 2. Обработать книгу с помощью mystem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "stem = Mystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* лемматизировать с помощью mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lemmatize = stem.lemmatize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* сохранить результат в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lemmatize_join = ''.join(text_lemmatize)\n",
    "\n",
    "with open('garri_potter_pymystem.txt', 'w') as outfile:\n",
    "    outfile.write(text_lemmatize_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* не на оценку, опционально: замерить время работы на примере небольшого кусочка текста (например, глава) (как замерить время см. ниже)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "JuUpQJemJf9Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.48 ms ± 125 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "m.lemmatize(annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzUUI4DzH9eG"
   },
   "source": [
    "### 3. Обработать книгу через pymorphy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* токенизировать текст с помощью nltk (см. конспект)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenize = word_tokenize(text)\n",
    "\n",
    "# токенизируем текст, приводим к нижнему регистру и оставляем только последовательности из букв\n",
    "text_tokenize = [w.lower() for w in text_tokenize if w.isalpha()]\n",
    "\n",
    "# фильтруем стопслова\n",
    "sw = stopwords.words('russian')\n",
    "text_tokenize = [w for w in text_tokenize if w not in sw]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* разобрать слова с помощью pymorphy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenize_morph = [morph.parse(w) for w in text_tokenize]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* сохраните результат в jsonlines: каждая строчка - это разбор слова в виде словаря {\"lemma\": \"конь\", \"word\": \"коня\", \"pos\": \"NOUN\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonlines = ''\n",
    "for parse in text_tokenize_morph:\n",
    "    jsonlines += str({\"lemma\": parse[0].normal_form, \"word\": parse[0].word, \"pos\": parse[0].tag.POS}) + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('garri_potter_pymorphy.jsonl', 'w') as infile:\n",
    "    infile.write(jsonlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* не на оценку, опционально: замерить время работы (как с майстемом)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385 µs ± 9.88 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "word_tokenize(annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zW6_NCEPH8kR"
   },
   "source": [
    "### 4. Ответить на вопросы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Какую долю слов составляет каждая часть речи? (Например, для глагола - это количество глаголов, деленное на общее число слов в тексте)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenize_morph_pos = [x[0].tag.POS for x in text_tokenize_morph]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN 0.41768412246071795\n",
      "PRTF 0.008366231796909724\n",
      "ADJF 0.10815969044942352\n",
      "NUMR 0.007895631258333551\n",
      "VERB 0.2546994692671704\n",
      "ADJS 0.00988261131009961\n",
      "PRCL 0.01911161076106565\n",
      "ADVB 0.07806740045491385\n",
      "PRTS 0.002980470077649089\n",
      "INTJ 0.003163481398206489\n",
      "INFN 0.03626238594473058\n",
      "CONJ 0.010222489476849068\n",
      "COMP 0.006588407540066407\n",
      "GRND 0.015634395670475046\n",
      "NPRO 0.012653925592825957\n",
      "PREP 0.007268163873565322\n",
      "PRED 0.0010980679233444013\n",
      "None 0.0002614447436534289\n"
     ]
    }
   ],
   "source": [
    "all_count = len(text_tokenize_morph_pos)\n",
    "pos_count = collections.Counter(text_tokenize_morph_pos)\n",
    "for pos, count in pos_count.items():\n",
    "    print(pos, count/all_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Найдите топ-20 (по частотности) глаголов и наречий (стоп слова можно убирать, можно и не убирать)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('сказать', 282), ('знать', 176), ('злеть', 131), ('дудлить', 127), ('видеть', 112), ('спросить', 107), ('хотеть', 99), ('ответить', 98), ('говорить', 93), ('мочь', 92), ('смотреть', 81), ('стать', 70), ('идти', 69), ('стоять', 56), ('дурслить', 56), ('посмотреть', 55), ('заметить', 55), ('уизлить', 54), ('быть', 54)]\n"
     ]
    }
   ],
   "source": [
    "text_tokenize_morph_verb = [x[0].normal_form for x in text_tokenize_morph if x[0].tag.POS == 'VERB' or x[0].tag.POS == 'INFN']\n",
    "verb_count = collections.Counter(text_tokenize_morph_verb)\n",
    "verb_count_sorted = sorted(verb_count.items(), key=lambda item: item[1])\n",
    "print(verb_count_sorted[:-20:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ещё', 182), ('очень', 91), ('пока', 76), ('едва', 52), ('снова', 49), ('быстро', 38), ('затем', 37), ('сегодня', 35), ('рядом', 33), ('тихо', 32), ('сразу', 32), ('несколько', 32), ('почему', 30), ('вместе', 29), ('прямо', 29), ('громко', 28), ('вообще', 27), ('точно', 27), ('сюда', 26)]\n"
     ]
    }
   ],
   "source": [
    "text_tokenize_morph_advb = [x[0].normal_form for x in text_tokenize_morph if x[0].tag.POS == 'ADVB']\n",
    "advb_count = collections.Counter(text_tokenize_morph_advb)\n",
    "advb_count_sorted = sorted(advb_count.items(), key=lambda item: item[1])\n",
    "print(advb_count_sorted[:-20:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWfbx-ywH7gl"
   },
   "source": [
    "### 5. Посмотрите документацию для nltk н-грамм..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрите документацию для nltk н-грамм (nltk.bigrams, например), попробуйте составить топ-25 биграмм и триграмм для вашего текста в лемматизированном виде (только леммы, без знаков препинания)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenize_lemmatize = stem.lemmatize(' '.join(text_tokenize))\n",
    "text_tokenize_lemmatize = list(filter(lambda x: x != ' ' and x != '\\n', text_tokenize_lemmatize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('дядя', 'вернон'), 104), (('профессор', 'макгонаголла'), 92), (('тетя', 'петуния'), 52), (('сказать', 'гарри'), 44), (('гарри', 'рон'), 39), (('волшебный', 'палочка'), 33), (('рон', 'гермиона'), 28), (('сказать', 'рон'), 27), (('спрашивать', 'гарри'), 27), (('гарри', 'поттер'), 27), (('отвечать', 'гарри'), 26), (('мистер', 'дурсло'), 26), (('гарри', 'гермиона'), 23), (('профессор', 'страунс'), 22), (('рон', 'гарри'), 20), (('миссис', 'дурслей'), 19), (('профессор', 'думбльдор'), 17), (('все', 'равный'), 17), (('сказать', 'огрид'), 16), (('друг', 'друг'), 16), (('гарри', 'знать'), 16), (('конец', 'конец'), 16), (('бирючинный', 'улица'), 16), (('миссис', 'норрис'), 15)]\n"
     ]
    }
   ],
   "source": [
    "text_tokenize_lemmatize_bigrams = list(nltk.bigrams(text_tokenize_lemmatize))\n",
    "bigrams_count = collections.Counter(text_tokenize_lemmatize_bigrams)\n",
    "bigrams_count_sorted = sorted(bigrams_count.items(), key=lambda item: item[1])\n",
    "print(bigrams_count_sorted[:-25:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('платформа', 'девять', 'четверть'), 8), (('гарри', 'рон', 'гермиона'), 6), (('сказать', 'дядя', 'вернон'), 6), (('коридор', 'третий', 'этаж'), 5), (('дядя', 'вернон', 'тетя'), 5), (('проходить', 'мимо', 'пушка'), 4), (('портрет', 'толстой', 'тетя'), 4), (('квидиш', 'сквозь', 'век'), 4), (('фред', 'джордж', 'уизли'), 4), (('гарри', 'покачать', 'голова'), 4), (('платформа', 'девять', 'десять'), 4), (('колдовство', 'ведьминский', 'искусство'), 4), (('школа', 'колдовство', 'ведьминский'), 4), (('вернон', 'тетя', 'петуния'), 4), (('гарри', 'дядя', 'вернон'), 4), (('сказать', 'профессор', 'макгонаголла'), 4), (('марс', 'сегодня', 'яркий'), 3), (('сто', 'пятьдесят', 'балл'), 3), (('потерять', 'дар', 'речь'), 3), (('профессор', 'макгонаголла', 'гарри'), 3), (('малфа', 'крабба', 'гойл'), 3), (('защита', 'сила', 'зло'), 3), (('гарри', 'профессор', 'макгонаголла'), 3), (('орешек', 'берти', 'ботт'), 3)]\n"
     ]
    }
   ],
   "source": [
    "text_tokenize_lemmatize_trigrams = list(nltk.trigrams(text_tokenize_lemmatize))\n",
    "trigrams_count = collections.Counter(text_tokenize_lemmatize_trigrams)\n",
    "trigrams_count_sorted = sorted(trigrams_count.items(), key=lambda item: item[1])\n",
    "print(trigrams_count_sorted[:-25:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему получаются именно такие?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основной причиной образования N-грамм является часто встречающаяся последовательность этих слов в тексте. Персонажи в одной сцене ('гарри', 'рон', 'гермиона'), описательные слова ('школа', 'колдовство', 'ведьминский'), стереотипные повадки ('гарри', 'покачать', 'голова') используюся в одном контексте чаще всего, как очень любимые автором."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_9OzZvQH6Qn"
   },
   "source": [
    "### 6. Возьмите абзац из изначального текста..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмите абзац из изначального текста (3 - 8 предложений), замените в нём время всех глаголов (например, прошедшее на настоящее или будущее), число всех существительных (единственное на множественное, множественное на единственное), сделайте согласование по числу глаголов и существительных. Например, вместо изначального Слон подарил мартышке цветы должно получиться Слоны подарят мартышкам цветок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changer(word):\n",
    "    w = word[0]\n",
    "    cases = list(w.tag.grammemes)[1:]\n",
    "    cases = list(map(lambda x: x.replace('sing', 'plur') if ('VERB' in cases or 'INFN' in cases or 'NOUN' in cases) and 'sing' in cases else x.replace('plur', 'sing'), cases))\n",
    "    cases = list(map(lambda x: x.replace('past', 'futr') if 'VERB' in cases and 'past' in cases else x.replace('futr', 'past'), cases))\n",
    "    if 'VERB' in cases:\n",
    "        cases.append('3per')\n",
    "    w = w.inflect(set(cases))\n",
    "    if not w:\n",
    "        w = word[0]\n",
    "    return w.word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Слон подарил мартышке цветы\n",
      "Слоны подарят мартышкам цветок\n"
     ]
    }
   ],
   "source": [
    "source_text = 'Слон подарил мартышке цветы'\n",
    "source_text_morph = [morph.parse(w) for w in source_text.split()]\n",
    "changed_words = list(map(changer, source_text_morph))\n",
    "changed_text = ' '.join(changed_words).capitalize()\n",
    "print(source_text)\n",
    "print(changed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Книга, покорившая мир, эталон литературы, синоним успеха. Книга, сделавшая Дж.К. Роулинг самым читаемым писателем современности. Книга, ставшая культовой уже для нескольких поколений. «Гарри Поттер и философский камень» — история начинается. Для среднего школьного возраста.\n",
      "Книга, покорившая мир, эталоны литературы, синонимы успеха. книга, сделавшая дж.к. роулинг самым читаемым писателями современности. книга, ставшая культовой уже для нескольких поколений. «гарри поттер и философский камень» — истории начинается. для средних школьного возраста.\n"
     ]
    }
   ],
   "source": [
    "source_text = annotation\n",
    "source_text_morph = [morph.parse(w) for w in source_text.split()]\n",
    "changed_words = list(map(changer, source_text_morph))\n",
    "changed_text = ' '.join(changed_words).capitalize()\n",
    "print(source_text)\n",
    "print(changed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далеко не все слова знакомы алгоритму pymorch, поэтому качество оставляет желать лучшего."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "a69ffce19cedea93f17373bbd46c91d9c1e83093e5157830401494233b428f48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
